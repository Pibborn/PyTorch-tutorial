{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch-tutorial-1.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Gy0IPS3JMRhS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# PyTorch Tutorial\n",
        "\n",
        "This is a short introduction to PyTorch that I am writing for various reasons. It will contain what I feel is useful to understand how to use PyTorch to train deep networks (MLPs, CNNs, RNNs).\n",
        "\n",
        "There are a few reasons to use PyTorch over TensorFlow; if you are interested in an in-depth comparison I am sure you will be able to find something on the web. My personal perspective is that PyTorch might be better as an educational and self-educational resource because of the following reasons:\n",
        "\n",
        "* **Multiple-level APIs**: do you want to write a new optimization algorithm? Use PyTorch's ```Tensor``` object and ignore everything else; need to perform visualization on weights and activation matrices? ```Tensor```s support Python-style indexing; want to try a new kind of architecture? Use the ```layers```-style APIs. You can use as little or as much of PyTorch as you want, and it still makes sense.\n",
        "* **Plays nice with Python**: writing TensorFlow code can be weird and convoluted because of the concept of ```Session``` and TF's ```Tensors``` having to be evaluated before having an actual value. This is because TensorFlow builds a static computational graph from your code, and each time you want to run the graph you have to interact with a ```Session```. On the other hand, PyTorch constructs a dynamic graph which is actually built again each time you want to perform a forward pass on your network: this means that all Python control statements can be used in your model definition. This is especially helpful if you are learning about RNNs.\n",
        "\n",
        "**Sources**:\n",
        "\n",
        "[Official tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)\n",
        "\n",
        "[Autograd whitepaper](https://openreview.net/pdf?id=BJJsrmfCZ)\n",
        "\n",
        "[Autograd documentation](https://pytorch.org/docs/stable/autograd.html)\n",
        "\n",
        "[pytorch-examples repository](https://github.com/jcjohnson/pytorch-examples) (very good)"
      ]
    },
    {
      "metadata": {
        "id": "BPwMG17orDFQ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# run this once\n",
        "!pip install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HS0K10XyNG5V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##PyTorch basics\n",
        "\n",
        "The core object in PyTorch is ```tensor```. If you are familiar with tensorflow, its role should not come as a surprise: it is the object that PyTorch uses to represent variables, data and just about everything numeric in your graph computation. The main difference is that PyTorch's ```tensor```s have a nicer API (opinion warning!) which make them easier to interact with numpy."
      ]
    },
    {
      "metadata": {
        "id": "sGpU89F5MFYE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## Hello, pytorch!\n",
        "!pip install torch\n",
        "\n",
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)\n",
        "z = x.numpy()\n",
        "print(z)\n",
        "\n",
        "## don't see the difference? try the following:\n",
        "print(type(x))\n",
        "print(type(z))\n",
        "\n",
        "## tensor slicing\n",
        "print(x[:, 2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GtQxTeR8OfKP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## Hello, tf!\n",
        "import tensorflow as tf\n",
        "\n",
        "x = tf.random_normal((5, 3))\n",
        "print(x)\n",
        "\n",
        "session = tf.Session()\n",
        "x_ = session.run(x)\n",
        "print(x_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cpz-J4ZJOGYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This allows us to discuss an important difference between ```tf``` and ```torch```. The first one uses a static computational graph, whereas ```torch``` uses a dynamic one. \n",
        "\n",
        "Simply put, this means that ```tf``` builds the computational graph only once; on the other hand, ```torch``` re-uses the user-provided graph definition to build the graph again and again. This has a few interesting repercussions:\n",
        "\n",
        "* *Lower efficiency*: on top of the graph building overhead, also take into account the automatic differentiation overhead -- if the graph is allowed to change from a forward pass to another, then all derivatives have to be computed again.\n",
        "* *Possibility to use standard language control flow*: you can use Python's control flow operators in your program: the tensors and functions they build will be differentiated nicely. On the other hand, in tensorflow you have ```tf.cond(...)``` and similar functions. There is no ```session``` concept in PyTorch.\n",
        "\n",
        "Note: it is not still apparent to me how exactly this second point follows from having a dynamic graph...\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AOxbI5GpiAwF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PyTorch gradient computation\n",
        "\n",
        "Each ```tensor``` object does not just contain a numpy array; on top of that, it has a few important attributes that are needed for gradient computation. Let's see a few concepts, without going into much detail:"
      ]
    },
    {
      "metadata": {
        "id": "_VDlsAV-cuTZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## User-defined tensors do not have grad_fn. This avoids the tf.placeholder() stuff you have in tf to feed data into your model.\n",
        "import torch\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "print(x.grad_fn)\n",
        "print(y.grad_fn)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YYfdjJ_nk_-w",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z)\n",
        "print(out)\n",
        "x.grad.data.zero_() ## try to comment this out if you are curious!\n",
        "out.backward()\n",
        "print(x.grad) ## d(out)/d(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vbHjON3soNGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Implementing a two-layer MLP with PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "5Xi9mmIMD5Zc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## random data\n",
        "\n",
        "num_examples = 1000\n",
        "input_size = 100\n",
        "num_classes = 10\n",
        "\n",
        "x = torch.randn(num_examples, input_size)\n",
        "y = torch.randn(num_examples, num_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gcf4DAMt9AA0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "## model definition and training\n",
        "\n",
        "H_size = 100\n",
        "\n",
        "w1 = torch.randn(input_size, H_size, requires_grad=True)\n",
        "b1 = torch.randn(H_size, requires_grad=True)\n",
        "w2 = torch.randn(H_size, num_classes, requires_grad=True)\n",
        "b2 = torch.randn(num_classes, requires_grad=True)\n",
        "\n",
        "num_iterations = 5\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for i in range(num_iterations):\n",
        "  o1 = torch.matmul(x, w1) + b1\n",
        "  o1 = torch.clamp(o1, min=0)\n",
        "  o2 = torch.matmul(o1, w2) + b2\n",
        "  y_pred = torch.clamp(o2, min=0)\n",
        "  loss = sum(sum((y_pred - y).pow(2)))\n",
        "  print(i, loss)\n",
        "\n",
        "  loss.backward()\n",
        "  #print(w1.grad[:2])\n",
        "  #print(w2.grad[:2])\n",
        "  with torch.no_grad():\n",
        "    w1 -= learning_rate * w1.grad\n",
        "    w2 -= learning_rate * w2.grad\n",
        "    b1 -= learning_rate * b1.grad\n",
        "    b2 -= learning_rate * b2.grad\n",
        "\n",
        "  # Manually zero the gradients after running the backward pass\n",
        "  w1.grad.zero_()\n",
        "  w2.grad.zero_()\n",
        "  b1.grad.zero_()\n",
        "  b2.grad.zero_()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PXwUBhpgQHH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The above, step by step \n",
        "\n",
        "Cell by cell, actually!"
      ]
    },
    {
      "metadata": {
        "id": "H4k-bpQTQERf",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# model definition\n",
        "H_size = 100\n",
        "\n",
        "w1 = torch.randn(input_size, H_size, requires_grad=True)\n",
        "b1 = torch.randn(H_size, requires_grad=True)\n",
        "w2 = torch.randn(H_size, num_classes, requires_grad=True)\n",
        "b2 = torch.randn(num_classes, requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dIg4imecLiNW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "learning_rate = 1e-8\n",
        "\n",
        "o1 = torch.matmul(x, w1) + b1\n",
        "o1 = torch.clamp(o1, min=0)\n",
        "o2 = torch.matmul(o1, w2) + b2\n",
        "y_pred = torch.clamp(o2, min=0)\n",
        "loss = sum(sum((y_pred - y).pow(2)))\n",
        "\n",
        "loss.backward(retain_graph=True)\n",
        "\n",
        "print(loss)\n",
        "\n",
        "with torch.no_grad():\n",
        "  w1 -= learning_rate * w1.grad\n",
        "  w2 -= learning_rate * w2.grad\n",
        "  \n",
        "print(w1.grad[0, 0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9tqjFWCQ2MR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# run this cell multiple times and watch the loss go down!\n",
        "\n",
        "loss.backward(retain_graph=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "  w1 -= learning_rate * w1.grad\n",
        "  w2 -= learning_rate * w2.grad\n",
        "  w1.grad.zero_()\n",
        "  w2.grad.zero_()\n",
        "  \n",
        "print(loss)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8_OOOOmLJAG2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The ``Sequential``` high-level APIs"
      ]
    },
    {
      "metadata": {
        "id": "fYJ5dQgd8g3u",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = torch.nn.Sequential(\n",
        "          torch.nn.Linear(input_size, H_size),\n",
        "          torch.nn.ReLU(),\n",
        "          torch.nn.Linear(H_size, num_classes),\n",
        "        )\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(size_average=False)\n",
        "learning_rate = 1e-8\n",
        "\n",
        "for i in range(100):\n",
        "  y_pred = model(x)\n",
        "  \n",
        "  loss = loss_fn(y_pred, y)\n",
        "  print(i, loss.item())\n",
        "  \n",
        "  model.zero_grad()\n",
        "  \n",
        "  loss.backward()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param.data -= learning_rate * param.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FNBz8wR9SGkY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}